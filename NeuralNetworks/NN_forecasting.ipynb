{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teoriniai klausimai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Palyginkite transformerius ir rekurentinius neuroninius tinklus laiko eilučių/sekų analizėje. Kokiais privalumais pasižymi transformeriai ir kaip tai pasiekiama (architektūra, mokymo algoritmai)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rekurentiniai neuroniniai tinklai (RNN) ir LSTM (Long Short-Term Memory), ilgą laiką buvo pagrindinis pasirinkimas sekų ir laiko eilučių analizėje dėl jų gebėjimo tvarkyti laikinę priklausomybę. RNN natūraliai pritaikyti sekoms, nes jie apdoroja įvestis žingsnis po žingsnio, išlaikydami vidines būsenas, kurios padeda modeliuoti laikinę priklausomybę. Be to, LSTM patobulinimas leidžia RNN geriau išsaugoti ir naudoti ilgalaikes priklausomybes, sumažinant problemą dėl dingstančio gradiento. Tačiau šie metodai pasižymi ir gana aiškias minusais, tai labai ilgų sekų apdorojimas, nes ilgalaikė priklausomybės problema išlieka. Dėl savo architektūros RNN nėra paralelizuojami, todėl ir apmokymo laikas išauga gan reikšmingai bei negalima pamiršti ir gradientų problemos, kada dėl sekos ilgumo treniruojant jie dingsta (sprogsta). \n",
    "\n",
    "Vienas iš pagrindinių transformerių privalumų yra jų gebėjimas paralelizuoti apdorojimą. Skirtingai nuo RNN, transformerių architektūra leidžia apdoroti visą seką vienu metu, o ne žingsnis po žingsnio. Tai leidžia žymiai greičiau mokytis. Be to, transformeriai gali efektyviai modeliuoti ilgalaikes priklausomybes, nes jų dėmesio mechanizmai leidžia kiekvienam įvesties elementui „matyti“ visą seką. Dėmesio mechanizmas (attention mechanism), įskaitant tiek multi-head mechanizmą tiek self-attention, leidžia modeliams skirti daugiau dėmesio svarbiems sekos elementams, gerinant modelio tikslumą ir interpretavimą. Transformerių mokymo algoritmai dažniausiai remiasi gradientų nusileidimu su papildomais metodais, tokiais kaip Adam optimizer, kuris dėl savo pritaikomumo ir greito konvergavimo dažnai naudojamas transformerių mokymui. Regularizacijos metodai, tokie kaip „dropout“, padeda išvengti persimokymo. Mokymo taktika, kaip „warm-up“ strategijos ir dinaminiai mokymosi greičio mažinimo planai, padeda stabilizuoti mokymo procesą."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Kodėl įprastinė svorių keitimo taisyklė (gradientinio nusileidimo) buvo papildyta sudėtingesniais svorių keitimo algoritmais (AdaGrad, RMSProp, Adam)? Kodėl šios taisyklės lemia geresnes mokymosi savybes? Kokius mokymosi algoritmus esate išbandę savo projektuose? Kokie Jūsų pastebėjimai?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Įprastinė gradientinio nusileidimo taisyklė buvo papildyta sudėtingesniais svorių keitimo algoritmais, tokiais kaip AdaGrad, RMSProp ir Adam, siekiant pagerinti mokymosi procesą ir pasiekti geresnius rezultatus. Tradicinis gradientinis nusileidimas dažnai susiduria su problemomis dėl nestabilaus konvergavimo ir užstrigimo vietiniuose minimumuose. Be to, dėl fiksuoto mokymosi greičio, gradientinis nusileidimas gali lėtai mokytis arba per greitai pereiti per minimumus, nepriklausomai nuo problemos sudėtingumo ar gradientų masto. Taip sudarant dar viena didelę problemą, jog mokymosi greitis turi būti parinktas labai tikslus, kad išvengti lokalaus minimumo arba globalaus minimumo nepasiekimo.\n",
    "\n",
    "Sudėtingesni svorių keitimo algoritmai, tokie kaip AdaGrad, RMSProp ir Adam, pagerina mokymosi savybes dėl jų gebėjimo pritaikyti mokymosi greitį dinaminiu būdu ir atsižvelgti į gradientų dydžius skirtingose ašyse. AdaGrad pritaiko mokymosi greitį pagal kiekvieno parametro gradientų istoriją. RMSProp ir Adam sprendžia AdaGrad problemas dėl per greito mokymosi greičio mažinimo, naudodami eksponentinį gradientų vidurkį, kuris leidžia efektyviau tvarkyti didelius gradientus ir stabilizuoja mokymosi procesą. Adam ypač populiarus dėl savo sujungtos AdaGrad ir RMSProp savybių, leidžiančių pasiekti greitą ir stabilų konvergavimą netgi sudėtingose problemose.\n",
    "\n",
    "Kadangi beveik visos mano užduotys yra susijusios su NLP, tai visad rinkausi Adam, nes kiti pasirinkimai atrodė svarbesni, tiek embeding parinkimas, tiek pačios architektūros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Palyginkite autoenkoderius ir generuojančius-priešiškus neuroninius tinklus: architektūra, nuostolių funkcija ir mokymo algoritmas, taikymo sritys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoenkoderiai yra neuroniniai tinklai, sudaryti iš dviejų pagrindinių dalių: enkoderio ir dekoderio. Enkoderis yra sluoksnių serija, kuri sumažina įvesties dimensijas, kurdama suspaustus pirminių duomenų atvaizdus, o dekoderis – sluoksnių serija, kuri bando atkurti pradinę įvestį iš sukurtų vaizdinių. Tuo tarpu GAN yra sudaryti iš dviejų pagrindinų komponenčių, tai generatoriaus ir diskriminatoriaus. Buitiškai kalbant generatorius bando sukurti duomenis artimus realiems, o diskriminatorius bando atskirti, kur yra realūs duomenys, o kada jam yra pateikama generatoriaus sugeneruotas dirbtinis pavyzdys.\n",
    "\n",
    "Autoenkoderiai dažniausia kaip nuostolių funkciją, kuri vertina kaip 'gerai' duomenys yra suspaudžiami, yra naudojama MSE (dar kartais ir MAE) bei entropija, kuri sulygina dviejų imčių pasiskirstymus. Mokymo algoritmas remiasi gradientiniu nusileidimu, optimizuojant nuostolių funkciją, kad minimizuotų atkūrimo klaidą. Tai leidžia autoenkoderiams mokytis efektyviai suspausti duomenis ir atkurti juos su minimaliu nuostoliu.\n",
    "\n",
    "GAN mokymo procese generatorius ir diskriminatorius treniruojami kartu, bet su skirtingais tikslais. Diskriminatorius bando kuo geriau atskirti tikrus duomenis nuo netikrų, o generatorius bando 'apgauti' diskriminatorių, sukuriant realistiškus pavyzdžius. Diskriminatoriaus nuostolių funkcija yra skaičiuojama atsižvelgiant į du pagrindinius aspektus tai į 'gerus' pavyzdžius, kurie gavo sugeneruoto atvėjo etiketę bei į 'blogus' pavyzdžius, kada generuotas atvėjis buvo klasifikuotas kaip realus. O generatoriaus atvėju, jis atsižvelgia ar sugebėjo 'suklaidinti' diskriminatorių ar ne. Mokymo algoritmas taip pat remiasi gradientiniu nusileidimu, tačiau generatorius ir diskriminatorius treniruojami pakaitomis, optimizuojant savo nuostolių funkcijas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Praktinėje dalyje komentuodami savo kodą pagrįskite savo teorinės dalies atsakymus. Jei praktinėje dalyje pagrįsti atsakymo negalite (pvz. nepanaudojote tokio neuroninio tinklo tipo) - pateikite savo minčių kaip reiktų pakoreguoti / pakeisti / papildyti duomenų rinkinį, kad tokį neuroninį tinklą būtų galima panaudoti."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praktinė dalis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duomenų rinkinys: https://data.gov.lt/datasets/2616/ \n",
    "Galutinis failo adresas: https://get.data.gov.lt/datasets/gov/nsa/sr/studiju_pertraukimas/Pertraukimas/:format/csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Savo hipotezei turite duomenų rinkinį susitvarkyti taip, kad turėtumėte laiko eilučių analizės uždavinį (būtina panaudoti RNN arba LSTM, ir bent vieną kitą neuroninį tinklą pvz. transformerius - tai jau jūsų pasirinkimas, tačiau negalima naudoti RNN ir LSTM, kaip dviejų skirtingų).\n",
    "2. Duomenys jau yra aprašyti, todėl galite duomenų aprašymą praleisti. Ataskaitoje pateikite hipotezę, uždavinius, paruoškite duomenis, stenkitės rasti tinkamiausią neuroninį tinklą (tarp skirtingų tipų, bei NN viduje tarp struktūros, parametrų) jūsų hipotezei pagrįsti / atmesti, komentuokite savo žingsnius, pateikite išvadas.\n",
    "\n",
    "Sėkmės!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egzaminas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "df = pd.read_csv('data/data_exam.csv')\n",
    "df['etapo_pradz_data'] = pd.to_datetime(df['etapo_pradz_data'], errors='coerce')\n",
    "df['grizimo_data'] = pd.to_datetime(df['grizimo_data'], errors='coerce')\n",
    "\n",
    "start_date = '2009-12-31'\n",
    "end_date = '2014-06-30'\n",
    "\n",
    "df = df[(df['etapo_pradz_data'] >= start_date) & (df['etapo_pradz_data'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 užduotis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kadangi reikia surasti laiko eilutę, tai pasirinksime sugrupuoti visus studentus, kurie grįžo į savo studijas pagal atitinkamus mėnesius ir paimsime ilgesnį laikotarpį, kad būtų galima gauti daugiau duomenų. Šiai užduočiai būtų galima pritaikyti nurodytus neuroninius tinklus ir šiai užduočiai pasirinksiu naudoti LSTM architektūrą. Taigi užduotis bus prognozuoti, kiek studentų turėtų grįžti į studijas ateinančiu laikotarpiu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Susiskaičiuojame, kiek studentų sugrįžta savo turimame laikotarpyje pagal mėnesį ir metus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to datetime with error handling\n",
    "date_columns = ['std_studiju_pradzia', 'stud_baigimo_data', 'grizimo_data', 'etapo_pradz_data']\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Define start and end dates\n",
    "start_date = min(df['grizimo_data'].min(), df['etapo_pradz_data'].min()).replace(day=1)\n",
    "end_date = pd.Timestamp('2015-06-30')\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='MS')\n",
    "\n",
    "# Precompute period format to avoid repetitive calls in the loop\n",
    "df['grizimo_month'] = df['grizimo_data'].dt.to_period('M')\n",
    "df['etapo_pradz_month'] = df['etapo_pradz_data'].dt.to_period('M')\n",
    "\n",
    "# Calculate results for each month\n",
    "results = [\n",
    "    {\n",
    "        'date': date,\n",
    "        'came_back_students': df[df['grizimo_month'] == date.to_period('M')].shape[0],\n",
    "        'left_students': df[df['etapo_pradz_month'] == date.to_period('M')].shape[0]\n",
    "    }\n",
    "    for date in date_range\n",
    "]\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.set_index('date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duomenų normalizavimas, siekiant turėti kuo mažiau trikdžiu apmokymo metu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(results_df[['left_students']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apsirašome, kokį modelį pritaikysime apmokymo bei prognozavimo užduočiai atlikti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(Model):\n",
    "    def __init__(self, sequence_length, units=50):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = LSTM(units=units, return_sequences=True, input_shape=(sequence_length, 1))\n",
    "        self.lstm2 = LSTM(units=units, return_sequences=True)\n",
    "        self.lstm3 = LSTM(units=units)\n",
    "        self.dense = Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.lstm3(x)\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 3\n",
    "x, y = [], []\n",
    "for i in range(sequence_length, len(scaled_data)):\n",
    "    x.append(scaled_data[i-sequence_length:i, 0])\n",
    "    y.append(scaled_data[i, 0])\n",
    "\n",
    "x, y = np.array(x), np.array(y)\n",
    "x = np.reshape(x, (x.shape[0], x.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apsirašome hyperparamterų ieškojimo funkciją, nurodant pirmines (o vėliau ir pakeistas) jos reikšmes  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    units = trial.suggest_int(\"units\", 10, 50, step=5)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 1, 4, step=1)\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 50, step=10)\n",
    "\n",
    "    model = LSTMModel(sequence_length=sequence_length, units=units)\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
    "    \n",
    "    model.fit(x, y, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    loss = model.evaluate(x, y, verbose=0)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-03 22:22:12,003] A new study created in memory with name: no-name-c29482f5-bbbe-4e3d-be4d-0d3f49aaaaa2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4n/7_w_ymgn12jccnsslx9l2ffr0000gn/T/ipykernel_16089/4228346410.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
      "[I 2024-11-03 22:22:16,338] Trial 0 finished with value: 0.941582977771759 and parameters: {'units': 50, 'learning_rate': 2.143849955451713e-05, 'batch_size': 4, 'epochs': 20}. Best is trial 0 with value: 0.941582977771759.\n",
      "[I 2024-11-03 22:22:20,756] Trial 1 finished with value: 0.6512454748153687 and parameters: {'units': 40, 'learning_rate': 0.0005926293478718205, 'batch_size': 3, 'epochs': 40}. Best is trial 1 with value: 0.6512454748153687.\n",
      "[I 2024-11-03 22:22:26,905] Trial 2 finished with value: 0.31450632214546204 and parameters: {'units': 50, 'learning_rate': 0.00843227329426804, 'batch_size': 1, 'epochs': 30}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:30,156] Trial 3 finished with value: 0.6710814833641052 and parameters: {'units': 35, 'learning_rate': 0.0007347322332356179, 'batch_size': 4, 'epochs': 10}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:37,276] Trial 4 finished with value: 0.9536621570587158 and parameters: {'units': 20, 'learning_rate': 3.7019541639935505e-05, 'batch_size': 2, 'epochs': 20}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:42,105] Trial 5 finished with value: 0.6629917025566101 and parameters: {'units': 40, 'learning_rate': 0.0002157593357424821, 'batch_size': 1, 'epochs': 20}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:45,902] Trial 6 finished with value: 0.6796796917915344 and parameters: {'units': 45, 'learning_rate': 0.00015661707814445687, 'batch_size': 2, 'epochs': 20}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:49,146] Trial 7 finished with value: 0.6785346865653992 and parameters: {'units': 25, 'learning_rate': 0.0006654982356635659, 'batch_size': 4, 'epochs': 10}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:56,665] Trial 8 finished with value: 0.6564005017280579 and parameters: {'units': 20, 'learning_rate': 0.00045900796688360827, 'batch_size': 1, 'epochs': 20}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:22:59,761] Trial 9 finished with value: 0.6600729823112488 and parameters: {'units': 30, 'learning_rate': 0.0015702310283878503, 'batch_size': 3, 'epochs': 10}. Best is trial 2 with value: 0.31450632214546204.\n",
      "[I 2024-11-03 22:23:07,462] Trial 10 finished with value: 0.1908854991197586 and parameters: {'units': 10, 'learning_rate': 0.009390789593558407, 'batch_size': 1, 'epochs': 50}. Best is trial 10 with value: 0.1908854991197586.\n",
      "[I 2024-11-03 22:23:15,106] Trial 11 finished with value: 0.1487453430891037 and parameters: {'units': 10, 'learning_rate': 0.008866307787397486, 'batch_size': 1, 'epochs': 50}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:20,606] Trial 12 finished with value: 0.22118018567562103 and parameters: {'units': 10, 'learning_rate': 0.009582933589502686, 'batch_size': 2, 'epochs': 50}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:28,234] Trial 13 finished with value: 0.32427269220352173 and parameters: {'units': 10, 'learning_rate': 0.0030001801919321922, 'batch_size': 1, 'epochs': 50}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:33,270] Trial 14 finished with value: 0.32797887921333313 and parameters: {'units': 15, 'learning_rate': 0.0034361671022515397, 'batch_size': 2, 'epochs': 40}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:40,245] Trial 15 finished with value: 0.3421885669231415 and parameters: {'units': 15, 'learning_rate': 0.0036224646356164372, 'batch_size': 1, 'epochs': 40}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:49,196] Trial 16 finished with value: 0.7099127173423767 and parameters: {'units': 10, 'learning_rate': 6.0445101391402694e-05, 'batch_size': 1, 'epochs': 50}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:53,597] Trial 17 finished with value: 0.4251803159713745 and parameters: {'units': 20, 'learning_rate': 0.0020660249306082333, 'batch_size': 3, 'epochs': 40}. Best is trial 11 with value: 0.1487453430891037.\n",
      "[I 2024-11-03 22:23:59,358] Trial 18 finished with value: 0.09036671370267868 and parameters: {'units': 25, 'learning_rate': 0.008462974861295311, 'batch_size': 2, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:03,934] Trial 19 finished with value: 0.6113951206207275 and parameters: {'units': 30, 'learning_rate': 0.0011587773445673071, 'batch_size': 2, 'epochs': 30}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:09,591] Trial 20 finished with value: 0.21085989475250244 and parameters: {'units': 25, 'learning_rate': 0.004949358276047554, 'batch_size': 2, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:17,248] Trial 21 finished with value: 0.17526690661907196 and parameters: {'units': 15, 'learning_rate': 0.006379189976863329, 'batch_size': 1, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:23,653] Trial 22 finished with value: 0.1991509050130844 and parameters: {'units': 15, 'learning_rate': 0.005829603354462608, 'batch_size': 1, 'epochs': 40}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:29,090] Trial 23 finished with value: 0.3368287682533264 and parameters: {'units': 25, 'learning_rate': 0.0018765310895798703, 'batch_size': 2, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:34,603] Trial 24 finished with value: 0.30263984203338623 and parameters: {'units': 15, 'learning_rate': 0.005513654641881604, 'batch_size': 1, 'epochs': 30}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:39,160] Trial 25 finished with value: 0.3104195296764374 and parameters: {'units': 20, 'learning_rate': 0.002898947081238939, 'batch_size': 3, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:48,270] Trial 26 finished with value: 0.22897738218307495 and parameters: {'units': 30, 'learning_rate': 0.005665106202322912, 'batch_size': 2, 'epochs': 40}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:24:56,090] Trial 27 finished with value: 0.4916450083255768 and parameters: {'units': 15, 'learning_rate': 0.000999281447939102, 'batch_size': 1, 'epochs': 50}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:25:02,914] Trial 28 finished with value: 0.9559292793273926 and parameters: {'units': 25, 'learning_rate': 1.0212082382767786e-05, 'batch_size': 1, 'epochs': 40}. Best is trial 18 with value: 0.09036671370267868.\n",
      "[I 2024-11-03 22:25:07,062] Trial 29 finished with value: 0.8077183961868286 and parameters: {'units': 10, 'learning_rate': 8.636893664350242e-05, 'batch_size': 2, 'epochs': 30}. Best is trial 18 with value: 0.09036671370267868.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'units': 25, 'learning_rate': 0.008462974861295311, 'batch_size': 2, 'epochs': 50}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasifikavimas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Šioje dalyje aprašysime, kaip galima panaudoti turimus duomenis ir sudarius klasifikavimo užduotį ją pateikti, kaip sprendimą norint prognozuoti, ar tam tikras studentas grįš į studijas ir jas tęs, ar jau tikėtis to neverta. Taip galbūt bandant prognozuoti, kokį krepšelį galima užtikrinti ir, kad galbūt laisvą vietą reiktų perduoti studentui, kuris tikrai mokysis, o rizikoje esančius tiesiog paskatinti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['std_pakopos_pav_code'] = pd.factorize(df['std_pakopos_pav'])[0]\n",
    "df['lytis_indicator'] = df['studento_lytis'].map({'V': 1, 'M': 0})\n",
    "df['ins_pagr_tipas_indicator'] = df['ins_pagr_tipas'].map({'Universitetas': 1, 'Kolegija': 0})\n",
    "df['Y'] = df['grizimo_data'].apply(lambda x: 1 if pd.notnull(x) else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "47117/47117 [==============================] - 29s 599us/step - loss: nan - accuracy: 0.7639 - val_loss: nan - val_accuracy: 0.7637\n",
      "Epoch 2/5\n",
      "47117/47117 [==============================] - 28s 595us/step - loss: nan - accuracy: 0.7640 - val_loss: nan - val_accuracy: 0.7637\n",
      "Epoch 3/5\n",
      "47117/47117 [==============================] - 29s 619us/step - loss: nan - accuracy: 0.7640 - val_loss: nan - val_accuracy: 0.7637\n",
      "Epoch 4/5\n",
      "47117/47117 [==============================] - 28s 593us/step - loss: nan - accuracy: 0.7640 - val_loss: nan - val_accuracy: 0.7637\n",
      "Epoch 5/5\n",
      "47117/47117 [==============================] - 28s 604us/step - loss: nan - accuracy: 0.7640 - val_loss: nan - val_accuracy: 0.7637\n",
      "921/921 [==============================] - 0s 385us/step\n",
      "Accuracy: 0.766341811266936\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87     22568\n",
      "           1       0.00      0.00      0.00      6881\n",
      "\n",
      "    accuracy                           0.77     29449\n",
      "   macro avg       0.38      0.50      0.43     29449\n",
      "weighted avg       0.59      0.77      0.66     29449\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "df['std_pakopos_pav_code'] = pd.factorize(df['std_pakopos_pav'])[0]\n",
    "df['lytis_indicator'] = df['studento_lytis'].map({'V': 1, 'M': 0})\n",
    "df['ins_pagr_tipas_indicator'] = df['ins_pagr_tipas'].map({'Universitetas': 1, 'Kolegija': 0})\n",
    "df['grizimo_data'] = pd.to_datetime(df['grizimo_data'], errors='coerce')\n",
    "df['grizimo_data_indicator'] = df['grizimo_data'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "\n",
    "X = df[['lytis_indicator', 'ins_pagr_tipas_indicator', 'std_pakopos_pav_code']]\n",
    "y = df['grizimo_data_indicator']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train_scaled.shape[1])),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=5, batch_size=2, validation_split=0.2)\n",
    "\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esant geresnėms sąlygoms, pavyzdžiui, naudojant galingesnę kompiuterinę įrangą, būtų galima atlikti daugiau eksperimentų ir optimizacijų, siekiant sukurti tikslesnį ir efektyvesnį modelį. Galima būtų eksperimentuoti su įvairiais learning rate parametrais, naudoti metodus kaip learning rate annealing, learning rate scheduling ar Cyclical Learning Rates (CLR), siekiant rasti optimalų learning rate, kuris pagerintų mokymosi procesą. Taip pat būtų galima išbandyti skirtingas neuroninių tinklų architektūras, įskaitant giliau sukonstruotus modelius su daugiau sluoksnių ar hibridinius modelius, kuriuose būtų naudojami LSTM ar konvoliuciniai neuroniniai tinklai (CNN). Be to, galima būtų eksperimentuoti su įvairiais mokymo parametrais, tokiais kaip batch size, epochų skaičius, siekiant rasti optimalų modelio sudėtingumo lygį ir pagerinti prognozavimo tikslumą.\n",
    "\n",
    "Taip pat būtų galima ir patikslinti užduotį bei papildyti parametrų kiekį ir vėliau surasti tinkamus parametrus geriausiai pognozei atlikti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
